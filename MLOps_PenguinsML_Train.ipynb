{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d7ia-6Sg7HSkiT5jbSsioMFesXRVx0Is",
      "authorship_tag": "ABX9TyNzKP5hGP8ZRPQ6HGK4tpER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javierhellch/MLOps/blob/main/MLOps_PenguinsML_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Penguin Species Classification - Production Training Pipeline\n",
        "Modelo: Random Forest (optimizado para evitar overfitting)\n"
      ],
      "metadata": {
        "id": "bKvrazd8Vs7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "oQXp-Bc7VsO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clonar_repo(repo_origen,repo_destino):\n",
        "  !git clone {repo_origen} {repo_destino}"
      ],
      "metadata": {
        "id": "XKO2-AlMXVb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clonar_repo(\"https://github.com/javierhellch/MLOps.git\",\"/content/MLOps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXrMMkQMXa2I",
        "outputId": "c6b67579-199d-405d-c878-da5dece1c0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/MLOps'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:  10% (1/10)\u001b[K\rremote: Compressing objects:  20% (2/10)\u001b[K\rremote: Compressing objects:  30% (3/10)\u001b[K\rremote: Compressing objects:  40% (4/10)\u001b[K\rremote: Compressing objects:  50% (5/10)\u001b[K\rremote: Compressing objects:  60% (6/10)\u001b[K\rremote: Compressing objects:  70% (7/10)\u001b[K\rremote: Compressing objects:  80% (8/10)\u001b[K\rremote: Compressing objects:  90% (9/10)\u001b[K\rremote: Compressing objects: 100% (10/10)\u001b[K\rremote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:   7% (1/14)\rReceiving objects:  14% (2/14)\rReceiving objects:  21% (3/14)\rReceiving objects:  28% (4/14)\rReceiving objects:  35% (5/14)\rReceiving objects:  42% (6/14)\rReceiving objects:  50% (7/14)\rReceiving objects:  57% (8/14)\rReceiving objects:  64% (9/14)\rReceiving objects:  71% (10/14)\rReceiving objects:  78% (11/14)\rReceiving objects:  85% (12/14)\rReceiving objects:  92% (13/14)\rReceiving objects: 100% (14/14)\rReceiving objects: 100% (14/14), 8.12 KiB | 8.12 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PenguinPipeline:\n",
        "    \"\"\"Pipeline completo de entrenamiento para clasificaciÃ³n de pingÃ¼inos\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, output_dir='./models', random_state=42):\n",
        "        self.data_path = data_path\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # Componentes\n",
        "        self.label_encoders = {}\n",
        "        self.target_encoder = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "        self.model = None\n",
        "\n",
        "        # Datos\n",
        "        self.raw_data = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.feature_names = None\n",
        "\n",
        "        # Resultados\n",
        "        self.results = {}\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Carga y limpieza inicial de datos\"\"\"\n",
        "        print(\"\\n[1/8] Cargando datos...\")\n",
        "        self.raw_data = pd.read_csv(self.data_path)\n",
        "\n",
        "        # Eliminar columnas irrelevantes\n",
        "        cols_to_drop = ['Unnamed: 0', 'year'] if 'Unnamed: 0' in self.raw_data.columns else ['year']\n",
        "        if 'year' in self.raw_data.columns:\n",
        "            self.raw_data = self.raw_data.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "        print(f\"âœ“ Datos cargados: {self.raw_data.shape}\")\n",
        "        print(f\"  Especies: {self.raw_data['species'].value_counts().to_dict()}\")\n",
        "\n",
        "        # AnÃ¡lisis de valores faltantes\n",
        "        missing = self.raw_data.isnull().sum()\n",
        "        missing = missing[missing > 0]\n",
        "        if len(missing) > 0:\n",
        "            print(f\"\\n  Valores faltantes detectados:\")\n",
        "            for col, count in missing.items():\n",
        "                pct = (count / len(self.raw_data) * 100)\n",
        "                print(f\"    â€¢ {col}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocesamiento: encoding, feature engineering, imputaciÃ³n\"\"\"\n",
        "        print(\"\\n[2/8] Preprocesando datos...\")\n",
        "        df = self.raw_data.copy()\n",
        "\n",
        "        # 1. Encoding de variables categÃ³ricas\n",
        "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            if col != 'species':\n",
        "                le = LabelEncoder()\n",
        "                mask = df[col].notna()\n",
        "                df.loc[mask, col] = le.fit_transform(df.loc[mask, col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "            else:\n",
        "                # Encodear el target\n",
        "                self.target_encoder = LabelEncoder()\n",
        "                df['species'] = self.target_encoder.fit_transform(df['species'])\n",
        "\n",
        "        print(f\"âœ“ Variables categÃ³ricas codificadas\")\n",
        "\n",
        "        # 2. Feature Engineering\n",
        "        df['bill_ratio'] = df['bill_length_mm'] / (df['bill_depth_mm'] + 1e-6)\n",
        "        df['body_mass_index'] = df['body_mass_g'] / (df['flipper_length_mm'] + 1e-6)\n",
        "        df['bill_size'] = df['bill_length_mm'] * df['bill_depth_mm']\n",
        "\n",
        "        print(f\"âœ“ Feature engineering completado (3 nuevas features)\")\n",
        "\n",
        "        # 3. Separar features y target\n",
        "        X = df.drop(columns=['species'])\n",
        "        y = df['species']\n",
        "\n",
        "        self.feature_names = X.columns.tolist()\n",
        "\n",
        "        # 4. ImputaciÃ³n KNN\n",
        "        X_imputed = self.imputer.fit_transform(X)\n",
        "        X_imputed = pd.DataFrame(X_imputed, columns=self.feature_names, index=X.index)\n",
        "\n",
        "        print(f\"âœ“ ImputaciÃ³n KNN completada\")\n",
        "\n",
        "        # 5. Train/Test Split estratificado\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X_imputed, y,\n",
        "            test_size=0.2,\n",
        "            random_state=self.random_state,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        print(f\"\\nâœ“ Split completado:\")\n",
        "        print(f\"  Train: {self.X_train.shape[0]} muestras\")\n",
        "        print(f\"  Test: {self.X_test.shape[0]} muestras\")\n",
        "\n",
        "        # 6. Scaling (DESPUÃ‰S del split para evitar data leakage)\n",
        "        self.X_train = pd.DataFrame(\n",
        "            self.scaler.fit_transform(self.X_train),\n",
        "            columns=self.feature_names,\n",
        "            index=self.X_train.index\n",
        "        )\n",
        "\n",
        "        self.X_test = pd.DataFrame(\n",
        "            self.scaler.transform(self.X_test),\n",
        "            columns=self.feature_names,\n",
        "            index=self.X_test.index\n",
        "        )\n",
        "\n",
        "        print(f\"âœ“ Escalado aplicado\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construye Random Forest con hiperparÃ¡metros CONSERVADORES\n",
        "        para evitar overfitting\n",
        "        \"\"\"\n",
        "        print(\"\\n[3/8] Construyendo modelo Random Forest...\")\n",
        "\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=100,           # Menos Ã¡rboles para evitar overfitting\n",
        "            max_depth=8,                # Profundidad limitada (conservador)\n",
        "            min_samples_split=10,       # MÃ­nimo alto para splits\n",
        "            min_samples_leaf=4,         # MÃ­nimo alto por hoja\n",
        "            max_features='sqrt',        # Solo sqrt(n) features por split\n",
        "            bootstrap=True,             # Bootstrap para diversidad\n",
        "            oob_score=True,             # Out-of-bag score para validaciÃ³n\n",
        "            class_weight='balanced',    # Maneja desbalance de clases\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        print(f\"âœ“ Random Forest creado con hiperparÃ¡metros conservadores:\")\n",
        "        print(f\"  â€¢ n_estimators: 100\")\n",
        "        print(f\"  â€¢ max_depth: 8 (evita Ã¡rboles muy profundos)\")\n",
        "        print(f\"  â€¢ min_samples_split: 10\")\n",
        "        print(f\"  â€¢ min_samples_leaf: 4\")\n",
        "        print(f\"  â€¢ max_features: sqrt\")\n",
        "        print(f\"  â€¢ class_weight: balanced\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Entrena el modelo y realiza validaciÃ³n cruzada\"\"\"\n",
        "        print(\"\\n[4/8] Entrenando modelo...\")\n",
        "\n",
        "        # Entrenar\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # Out-of-bag score (similar a validaciÃ³n cruzada)\n",
        "        oob_score = self.model.oob_score_\n",
        "        print(f\"âœ“ Modelo entrenado\")\n",
        "        print(f\"  â€¢ OOB Score: {oob_score:.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def cross_validate(self):\n",
        "        \"\"\"ValidaciÃ³n cruzada estratificada\"\"\"\n",
        "        print(\"\\n[5/8] ValidaciÃ³n cruzada (5-fold)...\")\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
        "        cv_scores = cross_val_score(\n",
        "            self.model, self.X_train, self.y_train,\n",
        "            cv=skf, scoring='accuracy', n_jobs=-1\n",
        "        )\n",
        "\n",
        "        self.results['cv_scores'] = cv_scores\n",
        "        self.results['cv_mean'] = cv_scores.mean()\n",
        "        self.results['cv_std'] = cv_scores.std()\n",
        "\n",
        "        print(f\"âœ“ Cross-validation completada:\")\n",
        "        print(f\"  â€¢ Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
        "        print(f\"  â€¢ Scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "\n",
        "        # Verificar varianza - alto std puede indicar overfitting\n",
        "        if cv_scores.std() > 0.05:\n",
        "            print(f\"  âš  DesviaciÃ³n estÃ¡ndar alta (>{0.05:.2f}) - posible overfitting\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"EvalÃºa el modelo en el conjunto de test\"\"\"\n",
        "        print(\"\\n[6/8] Evaluando en conjunto de test...\")\n",
        "\n",
        "        # Predicciones\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "        y_pred_proba = self.model.predict_proba(self.X_test)\n",
        "\n",
        "        # MÃ©tricas\n",
        "        self.results['test_accuracy'] = accuracy_score(self.y_test, y_pred)\n",
        "        self.results['test_f1'] = f1_score(self.y_test, y_pred, average='weighted')\n",
        "        self.results['confusion_matrix'] = confusion_matrix(self.y_test, y_pred)\n",
        "        self.results['classification_report'] = classification_report(\n",
        "            self.y_test, y_pred,\n",
        "            target_names=self.target_encoder.classes_\n",
        "        )\n",
        "\n",
        "        print(f\"\\nâœ“ Resultados en Test Set:\")\n",
        "        print(f\"  â€¢ Accuracy: {self.results['test_accuracy']:.4f}\")\n",
        "        print(f\"  â€¢ F1-Score: {self.results['test_f1']:.4f}\")\n",
        "\n",
        "        print(f\"\\n  Classification Report:\")\n",
        "        print(self.results['classification_report'])\n",
        "\n",
        "        print(f\"\\n  Confusion Matrix:\")\n",
        "        print(self.results['confusion_matrix'])\n",
        "\n",
        "        # Comparar train vs test (detectar overfitting)\n",
        "        train_accuracy = self.model.score(self.X_train, self.y_train)\n",
        "        gap = train_accuracy - self.results['test_accuracy']\n",
        "\n",
        "        print(f\"\\n  AnÃ¡lisis de Overfitting:\")\n",
        "        print(f\"  â€¢ Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"  â€¢ Test Accuracy: {self.results['test_accuracy']:.4f}\")\n",
        "        print(f\"  â€¢ Gap: {gap:.4f}\")\n",
        "\n",
        "        if gap > 0.05:\n",
        "            print(f\"  âš  Gap alto (>{0.05:.2f}) - posible overfitting\")\n",
        "        elif gap < 0.02:\n",
        "            print(f\"  âœ“ Gap bajo (<0.02) - buena generalizaciÃ³n\")\n",
        "        else:\n",
        "            print(f\"  âœ“ Gap moderado - generalizaciÃ³n aceptable\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Crea visualizaciones del modelo\"\"\"\n",
        "        print(\"\\n[7/8] Creando visualizaciones...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Random Forest - Production Model Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Feature Importance\n",
        "        ax1 = axes[0, 0]\n",
        "        importances = self.model.feature_importances_\n",
        "        indices = np.argsort(importances)[-10:]  # Top 10\n",
        "\n",
        "        ax1.barh(range(len(indices)), importances[indices], color='#4ECDC4')\n",
        "        ax1.set_yticks(range(len(indices)))\n",
        "        ax1.set_yticklabels([self.feature_names[i] for i in indices])\n",
        "        ax1.set_title('Top 10 Feature Importance', fontweight='bold')\n",
        "        ax1.set_xlabel('Importance')\n",
        "\n",
        "        # 2. Confusion Matrix\n",
        "        ax2 = axes[0, 1]\n",
        "        cm = self.results['confusion_matrix']\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
        "                    xticklabels=self.target_encoder.classes_,\n",
        "                    yticklabels=self.target_encoder.classes_)\n",
        "        ax2.set_title('Confusion Matrix (Test Set)', fontweight='bold')\n",
        "        ax2.set_ylabel('True Label')\n",
        "        ax2.set_xlabel('Predicted Label')\n",
        "\n",
        "        # 3. Train vs Test Accuracy (Overfitting check)\n",
        "        ax3 = axes[1, 0]\n",
        "        train_acc = self.model.score(self.X_train, self.y_train)\n",
        "        test_acc = self.results['test_accuracy']\n",
        "        cv_acc = self.results['cv_mean']\n",
        "\n",
        "        metrics = ['Train', 'CV (5-fold)', 'Test']\n",
        "        accuracies = [train_acc, cv_acc, test_acc]\n",
        "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "\n",
        "        bars = ax3.bar(metrics, accuracies, color=colors, alpha=0.8)\n",
        "        ax3.set_ylim([0.85, 1.0])\n",
        "        ax3.set_title('Accuracy Comparison (Overfitting Check)', fontweight='bold')\n",
        "        ax3.set_ylabel('Accuracy')\n",
        "        ax3.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='Target: 95%')\n",
        "\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
        "                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        ax3.legend()\n",
        "\n",
        "        # 4. Cross-Validation Scores\n",
        "        ax4 = axes[1, 1]\n",
        "        cv_scores = self.results['cv_scores']\n",
        "        folds = [f'Fold {i+1}' for i in range(len(cv_scores))]\n",
        "\n",
        "        ax4.bar(folds, cv_scores, color='#96CEB4', alpha=0.8)\n",
        "        ax4.axhline(y=cv_scores.mean(), color='red', linestyle='--',\n",
        "                   label=f'Mean: {cv_scores.mean():.4f}')\n",
        "        ax4.fill_between(range(len(cv_scores)),\n",
        "                        cv_scores.mean() - cv_scores.std(),\n",
        "                        cv_scores.mean() + cv_scores.std(),\n",
        "                        alpha=0.2, color='red', label=f'Â±1 STD: {cv_scores.std():.4f}')\n",
        "        ax4.set_title('Cross-Validation Scores (Stability Check)', fontweight='bold')\n",
        "        ax4.set_ylabel('Accuracy')\n",
        "        ax4.set_ylim([0.90, 1.0])\n",
        "        ax4.legend()\n",
        "        ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        viz_path = self.output_dir / 'model_analysis.png'\n",
        "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"âœ“ VisualizaciÃ³n guardada: {viz_path}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Guarda el modelo y todos los artefactos necesarios para producciÃ³n\"\"\"\n",
        "        print(\"\\n[8/8] Guardando modelo y artefactos...\")\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # 1. Guardar modelo\n",
        "        model_path = self.output_dir / 'penguin_model.pkl'\n",
        "        with open(model_path, 'wb') as f:\n",
        "            pickle.dump(self.model, f)\n",
        "\n",
        "        # 2. Guardar transformadores\n",
        "        transformers = {\n",
        "            'label_encoders': self.label_encoders,\n",
        "            'target_encoder': self.target_encoder,\n",
        "            'scaler': self.scaler,\n",
        "            'imputer': self.imputer,\n",
        "            'feature_names': self.feature_names\n",
        "        }\n",
        "\n",
        "        transformer_path = self.output_dir / 'transformers.pkl'\n",
        "        with open(transformer_path, 'wb') as f:\n",
        "            pickle.dump(transformers, f)\n",
        "\n",
        "        # 3. Guardar metadata\n",
        "        metadata = {\n",
        "            'model_type': 'RandomForestClassifier',\n",
        "            'timestamp': timestamp,\n",
        "            'train_date': datetime.now().isoformat(),\n",
        "            'random_state': self.random_state,\n",
        "            'hyperparameters': {\n",
        "                'n_estimators': self.model.n_estimators,\n",
        "                'max_depth': self.model.max_depth,\n",
        "                'min_samples_split': self.model.min_samples_split,\n",
        "                'min_samples_leaf': self.model.min_samples_leaf,\n",
        "                'max_features': self.model.max_features,\n",
        "                'class_weight': 'balanced'\n",
        "            },\n",
        "            'performance': {\n",
        "                'test_accuracy': float(self.results['test_accuracy']),\n",
        "                'test_f1_score': float(self.results['test_f1']),\n",
        "                'cv_accuracy_mean': float(self.results['cv_mean']),\n",
        "                'cv_accuracy_std': float(self.results['cv_std']),\n",
        "                'oob_score': float(self.model.oob_score_),\n",
        "                'train_accuracy': float(self.model.score(self.X_train, self.y_train)),\n",
        "                'overfitting_gap': float(self.model.score(self.X_train, self.y_train) - self.results['test_accuracy'])\n",
        "            },\n",
        "            'data_info': {\n",
        "                'total_samples': len(self.raw_data),\n",
        "                'train_samples': len(self.X_train),\n",
        "                'test_samples': len(self.X_test),\n",
        "                'n_features': len(self.feature_names),\n",
        "                'target_classes': self.target_encoder.classes_.tolist()\n",
        "            },\n",
        "            'feature_names': self.feature_names\n",
        "        }\n",
        "\n",
        "        metadata_path = self.output_dir / 'model_metadata.json'\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ Modelo guardado: {model_path}\")\n",
        "        print(f\"âœ“ Transformadores guardados: {transformer_path}\")\n",
        "        print(f\"âœ“ Metadata guardada: {metadata_path}\")\n",
        "\n",
        "        # 4. Crear archivo de configuraciÃ³n para FastAPI\n",
        "        config = {\n",
        "            'model_path': 'models/penguin_model.pkl',\n",
        "            'transformers_path': 'models/transformers.pkl',\n",
        "            'metadata_path': 'models/model_metadata.json'\n",
        "        }\n",
        "\n",
        "        config_path = self.output_dir / 'config.json'\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        print(f\"âœ“ ConfiguraciÃ³n guardada: {config_path}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Ejecuta el pipeline completo\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PENGUIN CLASSIFICATION - PRODUCTION TRAINING PIPELINE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self.load_data()\n",
        "        self.preprocess_data()\n",
        "        self.build_model()\n",
        "        self.train_model()\n",
        "        self.cross_validate()\n",
        "        self.evaluate_model()\n",
        "        self.create_visualizations()\n",
        "        self.save_model()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"âœ“ PIPELINE COMPLETADO EXITOSAMENTE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Resumen final\n",
        "        overfitting_gap = self.model.score(self.X_train, self.y_train) - self.results['test_accuracy']\n",
        "\n",
        "        print(f\"\\nðŸ“Š RESUMEN:\")\n",
        "        print(f\"  â€¢ Modelo: Random Forest (conservador)\")\n",
        "        print(f\"  â€¢ Test Accuracy: {self.results['test_accuracy']:.4f}\")\n",
        "        print(f\"  â€¢ CV Accuracy: {self.results['cv_mean']:.4f} Â± {self.results['cv_std']:.4f}\")\n",
        "        print(f\"  â€¢ OOB Score: {self.model.oob_score_:.4f}\")\n",
        "        print(f\"  â€¢ Overfitting Gap: {overfitting_gap:.4f}\")\n",
        "\n",
        "        if overfitting_gap < 0.02:\n",
        "            print(f\"\\nâœ“ Modelo con buena generalizaciÃ³n - LISTO PARA PRODUCCIÃ“N\")\n",
        "        elif overfitting_gap < 0.05:\n",
        "            print(f\"\\nâœ“ Modelo con generalizaciÃ³n aceptable - APTO PARA PRODUCCIÃ“N\")\n",
        "        else:\n",
        "            print(f\"\\nâš  Modelo con posible overfitting - REVISAR ANTES DE PRODUCCIÃ“N\")\n",
        "\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "VVTXfMTnVN_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Entrenar modelo\n",
        "    pipeline = PenguinPipeline(\n",
        "        data_path='/content/MLOps/PenguinsML/penguins.csv',\n",
        "        output_dir='/content/MLOps/PenguinsML/Training'\n",
        "    )\n",
        "\n",
        "    model = pipeline.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyG8IvaEXK4j",
        "outputId": "c243747f-e7d2-4efe-ce00-f7a043874990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PENGUIN CLASSIFICATION - PRODUCTION TRAINING PIPELINE\n",
            "======================================================================\n",
            "\n",
            "[1/8] Cargando datos...\n",
            "âœ“ Datos cargados: (344, 7)\n",
            "  Especies: {'Adelie': 152, 'Gentoo': 124, 'Chinstrap': 68}\n",
            "\n",
            "  Valores faltantes detectados:\n",
            "    â€¢ bill_length_mm: 2 (0.58%)\n",
            "    â€¢ bill_depth_mm: 2 (0.58%)\n",
            "    â€¢ flipper_length_mm: 2 (0.58%)\n",
            "    â€¢ body_mass_g: 2 (0.58%)\n",
            "    â€¢ sex: 11 (3.20%)\n",
            "\n",
            "[2/8] Preprocesando datos...\n",
            "âœ“ Variables categÃ³ricas codificadas\n",
            "âœ“ Feature engineering completado (3 nuevas features)\n",
            "âœ“ ImputaciÃ³n KNN completada\n",
            "\n",
            "âœ“ Split completado:\n",
            "  Train: 275 muestras\n",
            "  Test: 69 muestras\n",
            "âœ“ Escalado aplicado\n",
            "\n",
            "[3/8] Construyendo modelo Random Forest...\n",
            "âœ“ Random Forest creado con hiperparÃ¡metros conservadores:\n",
            "  â€¢ n_estimators: 100\n",
            "  â€¢ max_depth: 8 (evita Ã¡rboles muy profundos)\n",
            "  â€¢ min_samples_split: 10\n",
            "  â€¢ min_samples_leaf: 4\n",
            "  â€¢ max_features: sqrt\n",
            "  â€¢ class_weight: balanced\n",
            "\n",
            "[4/8] Entrenando modelo...\n",
            "âœ“ Modelo entrenado\n",
            "  â€¢ OOB Score: 0.9818\n",
            "\n",
            "[5/8] ValidaciÃ³n cruzada (5-fold)...\n",
            "âœ“ Cross-validation completada:\n",
            "  â€¢ Accuracy: 0.9782 Â± 0.0178\n",
            "  â€¢ Scores: ['0.9636', '0.9636', '1.0000', '0.9636', '1.0000']\n",
            "\n",
            "[6/8] Evaluando en conjunto de test...\n",
            "\n",
            "âœ“ Resultados en Test Set:\n",
            "  â€¢ Accuracy: 0.9855\n",
            "  â€¢ F1-Score: 0.9856\n",
            "\n",
            "  Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      0.97      0.98        30\n",
            "   Chinstrap       0.93      1.00      0.97        14\n",
            "      Gentoo       1.00      1.00      1.00        25\n",
            "\n",
            "    accuracy                           0.99        69\n",
            "   macro avg       0.98      0.99      0.98        69\n",
            "weighted avg       0.99      0.99      0.99        69\n",
            "\n",
            "\n",
            "  Confusion Matrix:\n",
            "[[29  1  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 25]]\n",
            "\n",
            "  AnÃ¡lisis de Overfitting:\n",
            "  â€¢ Train Accuracy: 0.9927\n",
            "  â€¢ Test Accuracy: 0.9855\n",
            "  â€¢ Gap: 0.0072\n",
            "  âœ“ Gap bajo (<0.02) - buena generalizaciÃ³n\n",
            "\n",
            "[7/8] Creando visualizaciones...\n",
            "âœ“ VisualizaciÃ³n guardada: /content/MLOps/PenguinsML/Training/model_analysis.png\n",
            "\n",
            "[8/8] Guardando modelo y artefactos...\n",
            "âœ“ Modelo guardado: /content/MLOps/PenguinsML/Training/penguin_model.pkl\n",
            "âœ“ Transformadores guardados: /content/MLOps/PenguinsML/Training/transformers.pkl\n",
            "âœ“ Metadata guardada: /content/MLOps/PenguinsML/Training/model_metadata.json\n",
            "âœ“ ConfiguraciÃ³n guardada: /content/MLOps/PenguinsML/Training/config.json\n",
            "\n",
            "======================================================================\n",
            "âœ“ PIPELINE COMPLETADO EXITOSAMENTE\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š RESUMEN:\n",
            "  â€¢ Modelo: Random Forest (conservador)\n",
            "  â€¢ Test Accuracy: 0.9855\n",
            "  â€¢ CV Accuracy: 0.9782 Â± 0.0178\n",
            "  â€¢ OOB Score: 0.9818\n",
            "  â€¢ Overfitting Gap: 0.0072\n",
            "\n",
            "âœ“ Modelo con buena generalizaciÃ³n - LISTO PARA PRODUCCIÃ“N\n"
          ]
        }
      ]
    }
  ]
}